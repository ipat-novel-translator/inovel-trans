#!/usr/bin/env python3
"""
inovel-trans.py - ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏à‡∏µ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: Gemini 2.5 Flash/Pro + Fallback + .env + metadata
"""

import os
import sys
import re
import json
import time
import shutil
import logging
import datetime
import random
import subprocess
from typing import Dict, List, Any, Tuple
import structlog
import yaml
import argparse

# ‡πÇ‡∏´‡∏•‡∏î .env
from dotenv import load_dotenv
load_dotenv()

# Aho-Corasick (optional)
try:
    import ahocorasick
    AHO_AVAILABLE = True
except ImportError:
    AHO_AVAILABLE = False

# Google Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    print("ERROR: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á google-generativeai: pip install -U google-generativeai")
    sys.exit(1)

# tiktoken (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì token)
try:
    import tiktoken
    HAVE_TIKTOKEN = True
except ImportError:
    HAVE_TIKTOKEN = False


# ---------------------------
# üîí Custom Exception
# ---------------------------
class SafetyBlockError(Exception):
    """Raised when Gemini blocks content due to safety filters"""
    pass


# ---------------------------
# üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
# ---------------------------
def get_available_models(preferred_models: List[str]) -> List[str]:
    """‡∏Ñ‡∏∑‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö preferred)"""
    try:
        available = set()
        for m in genai.list_models():
            if 'generateContent' in (m.supported_generation_methods or []):
                name = m.name.replace("models/", "")
                available.add(name)
        structlog.get_logger().debug("Available models", models=sorted(available))
    except Exception as e:
        structlog.get_logger().warning("Cannot fetch model list", error=str(e))
        # ‡πÉ‡∏ä‡πâ whitelist ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        available = {
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
            "gemini-2.5-pro"
        }

    filtered = [model.replace("models/", "") for model in preferred_models if model.replace("models/", "") in available]
    if not filtered:
        fallback = ["gemini-1.5-flash"]
        structlog.get_logger().error("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£! ‡πÉ‡∏ä‡πâ fallback", fallback=fallback)
        return fallback
    return filtered


# ---------------------------
# üîß Configuration
# ---------------------------
class Config:
    def __init__(self):
        raw_model = os.getenv(
            "TRANSLATION_MODEL",
            "gemini-2.5-flash,gemini-2.5-flash-lite,gemini-2.0-flash,gemini-2.5-pro"
        )
        self.preferred_models = [m.strip() for m in raw_model.split(",")]
        self.available_models = get_available_models(self.preferred_models)
        self.model = self.available_models[0]

        self.token_limit = int(os.getenv("TOKEN_LIMIT", "8000"))
        # ‚úÖ ‡∏•‡∏î‡∏Ñ‡πà‡∏≤ default ‡πÄ‡∏õ‡πá‡∏ô 1500
        self.chunk_token_target = int(os.getenv("CHUNK_TOKEN_TARGET", "1500"))
        self.token_buffer = int(os.getenv("TOKEN_BUFFER", "400"))

        self.src_dir = os.getenv("SRC_DIR", "chapters_src")
        self.out_dir = os.getenv("OUT_DIR", "chapters_translated")
        self.log_dir = os.getenv("LOG_DIR", "logs")
        self.backup_dir = os.getenv("BACKUP_DIR", "backups")
        self.glossary_file = os.getenv("GLOSSARY_FILE", "glossary.yaml")
        self.cache_db = os.path.join(self.log_dir, "translation_cache.db")

        for path in [self.src_dir, self.out_dir, self.log_dir, self.backup_dir]:
            os.makedirs(path, exist_ok=True)


config = Config()


# ---------------------------
# Logging
# ---------------------------
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)
logger = structlog.get_logger("ip_trans")


# ---------------------------
# üîå ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API
# ---------------------------
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ Vertex AI ‡∏´‡∏£‡∏∑‡∏≠ AI Studio
if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    # ‡πÉ‡∏ä‡πâ Vertex AI (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á GEMINI_API_KEY)
    logger.info("‡πÉ‡∏ä‡πâ Vertex AI", credentials=os.getenv("GOOGLE_APPLICATION_CREDENTIALS"))
else:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        logger.error("‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡∏´‡∏£‡∏∑‡∏≠ GOOGLE_APPLICATION_CREDENTIALS ‡πÉ‡∏ô .env")
        sys.exit(1)
    genai.configure(api_key=GEMINI_API_KEY)


# ---------------------------
# üíæ Cache System
# ---------------------------
import sqlite3

def init_cache_db():
    with sqlite3.connect(config.cache_db) as conn:
        conn.execute('''
            CREATE TABLE IF NOT EXISTS translations (
                hanzi TEXT PRIMARY KEY,
                thai TEXT NOT NULL,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()

translation_cache = {}

def load_cache():
    translation_cache.clear()
    try:
        with sqlite3.connect(config.cache_db) as conn:
            cursor = conn.execute("SELECT hanzi, thai FROM translations")
            for row in cursor.fetchall():
                translation_cache[row[0]] = row[1]
        logger.info("Cache ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß", terms=len(translation_cache))
    except Exception as e:
        logger.warning("‡πÇ‡∏´‡∏•‡∏î cache ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", error=str(e))

def save_cache():
    try:
        with sqlite3.connect(config.cache_db) as conn:
            cursor = conn.cursor()
            for hanzi, thai in translation_cache.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO translations (hanzi, thai, updated_at)
                    VALUES (?, ?, CURRENT_TIMESTAMP)
                ''', (hanzi, thai))
            conn.commit()
    except Exception as e:
        logger.warning("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cache ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", error=str(e))


# ---------------------------
# üîÑ Retry Mechanism
# ---------------------------
def gemini_with_retry(call_func, max_retries=3):
    for i in range(max_retries):
        try:
            return call_func()
        except Exception as e:
            if i == max_retries - 1:
                raise
            err_msg = str(e).lower()
            if any(kw in err_msg for kw in ["rate limit", "timeout", "quota", "502", "503"]):
                wait = (2 ** i) + random.uniform(0, 1)
                logger.warning("‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà", attempt=i+1, wait=round(wait,1))
                time.sleep(wait)
            else:
                raise


# ---------------------------
# ‚ú® ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö model_name ‡πÑ‡∏î‡πâ
# ---------------------------
def translate_with_gemini(
    prompt: str,
    model_name: str,
    max_tokens: int = 2000,
    temperature: float = 0.2
) -> str:
    def make_call():
        model = genai.GenerativeModel(
            model_name,
            system_instruction="‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏à‡∏µ‡∏ô‡πÇ‡∏ö‡∏£‡∏≤‡∏ì ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÅ‡∏õ‡∏•‡∏ï‡∏£‡∏á‡πÜ ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö",
            safety_settings=[
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            ]
        )
        return model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                top_p=0.95,
                top_k=40
            )
        )
    
    resp = gemini_with_retry(make_call)
    
    if not resp.candidates:
        raise ValueError(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•. finish_reason: {resp.prompt_feedback}")
    
    candidate = resp.candidates[0]
    if not candidate.content.parts:
        reason = candidate.finish_reason
        reason_name = {1: "STOP", 2: "SAFETY", 3: "RECITATION", 4: "OTHER"}.get(reason, reason)
        if reason == 2:  # SAFETY
            raise SafetyBlockError(f"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ safety filter (finish_reason: {reason_name})")
        else:
            raise ValueError(f"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏á. finish_reason: {reason_name} ({reason})")
    
    return candidate.content.parts[0].text.strip()


# ---------------------------
# üîÑ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô fallback ‡πÅ‡∏ö‡∏ö‡∏â‡∏•‡∏≤‡∏î
# ---------------------------
def translate_with_model_fallback(
    prompt: str,
    preferred_models: List[str],
    max_tokens: int = 2000,
    temperature: float = 0.2
) -> Tuple[str, str]:
    """
    ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏•, ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
    """
    for model in preferred_models:
        try:
            logger.info("‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•", model=model)
            translation = translate_with_gemini(
                prompt=prompt,
                model_name=model,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return translation, model
        except SafetyBlockError as e:
            logger.warning("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ safety ‚Äî ‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ", model=model, error=str(e))
            continue  # ‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ!
        except Exception as e:
            logger.error("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏≠‡∏∑‡πà‡∏ô ‚Äî ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ", model=model, error=str(e))
            raise  # ‡πÑ‡∏°‡πà fallback ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô error ‡∏≠‡∏∑‡πà‡∏ô (‡πÄ‡∏ä‡πà‡∏ô network, quota)

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ = ‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ safety
    raise SafetyBlockError("‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ safety filters ‚Äî ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡πÑ‡∏î‡πâ")


# ---------------------------
# Utilities
# ---------------------------
def now_ts():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def backup_glossary(path=config.glossary_file):
    if os.path.exists(path):
        ts = now_ts()
        dst = os.path.join(config.backup_dir, f"{os.path.basename(path)}.{ts}.bak")
        shutil.copy(path, dst)
        logger.info("Glossary backed up", src=path, dst=dst)


# ---------------------------
# Load Glossary
# ---------------------------
def load_glossary() -> Dict[str, str]:
    if not os.path.exists(config.glossary_file):
        logger.info("No glossary found", path=config.glossary_file)
        return {}

    with open(config.glossary_file, "r", encoding="utf-8") as f:
        try:
            data = yaml.safe_load(f) or {}
            simple = {}
            for k, v in data.items():
                if isinstance(v, dict):
                    if "th" in v:
                        simple[k] = v["th"]
                elif isinstance(v, str):
                    simple[k] = v
            return simple
        except Exception as e:
            logger.error("Load glossary failed", error=str(e))
            raise


# ---------------------------
# Save Glossary
# ---------------------------
def save_glossary(glossary_dict: Dict[str, str], chapter_num: int = None):
    path = config.glossary_file
    current_data = {}
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                current_data = yaml.safe_load(f) or {}
        except Exception as e:
            logger.warning("Could not read existing glossary", error=str(e))

    added_count = 0
    updated_count = 0

    for hanzi, thai in glossary_dict.items():
        if hanzi in current_data:
            entry = current_data[hanzi]
            if isinstance(entry, dict):
                if entry.get("source_type") == "manual":
                    logger.debug("Skip update (manual override)", term=hanzi, old=entry.get("th"), new=thai)
                    continue
                else:
                    current_data[hanzi]["th"] = thai
                    if chapter_num:
                        current_data[hanzi]["chapter_first_seen"] = min(
                            current_data[hanzi].get("chapter_first_seen", chapter_num),
                            chapter_num
                        )
                    current_data[hanzi]["added_at"] = now_ts()
                    updated_count += 1
            else:
                current_data[hanzi] = {
                    "th": thai,
                    "chapter_first_seen": chapter_num,
                    "added_at": now_ts(),
                    "source_type": "auto",
                    "notes": ""
                }
                updated_count += 1
        else:
            current_data[hanzi] = {
                "th": thai,
                "chapter_first_seen": chapter_num,
                "added_at": now_ts(),
                "source_type": "auto",
                "notes": ""
            }
            added_count += 1

    backup_glossary(path)
    with open(path, "w", encoding="utf-8") as f:
        yaml.dump(current_data, f, allow_unicode=True, sort_keys=False, indent=2)

    logger.info("Saved glossary", total=len(current_data), added=added_count, updated=updated_count, path=path)


# ---------------------------
# Aho-Corasick Utils
# ---------------------------
def build_automaton_from_dict(word_dict: dict):
    if not AHO_AVAILABLE:
        return None
    auto = ahocorasick.Automaton()
    for word in word_dict:
        auto.add_word(word, word)
    auto.make_automaton()
    return auto

def find_all_matched_terms(text: str, automaton) -> set:
    if automaton is None:
        return set()
    return {item[1] for item in automaton.iter(text)}


# ---------------------------
# Extract candidates (‡πÉ‡∏ä‡πâ Gemini)
# ---------------------------
def extract_candidates(text: str, existing_glossary: Dict[str, str]) -> List[str]:
    existing_keys = set(existing_glossary.keys())
    truncated_text = text[:4000]

    prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏à‡∏µ‡∏ô‡πÇ‡∏ö‡∏£‡∏≤‡∏ì‡πÅ‡∏ô‡∏ß‡πÑ‡∏ã‡πà‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô "
        "‡πÉ‡∏´‡πâ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏ô‡∏±‡∏Å ‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡∏Ç‡∏±‡πâ‡∏ô‡∏û‡∏•‡∏±‡∏á) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ "
        "‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô [\"‡∏Ñ‡∏≥1\", \"‡∏Ñ‡∏≥2\"] "
        "‡∏≠‡∏¢‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤\n\n"
        f"[‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß - ‡∏≠‡∏¢‡πà‡∏≤‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô]\n{list(existing_keys)}\n\n"
        f"[‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö]\n{truncated_text}\n\n"
        "‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON array ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏´‡∏°‡πà:"
    )

    try:
        response_text = translate_with_gemini(
            prompt=prompt,
            model_name=config.model,  # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract
            max_tokens=300,
            temperature=0.0
        )

        try:
            data = json.loads(response_text)
        except json.JSONDecodeError:
            data = extract_json_safely(response_text)

        if isinstance(data, list):
            candidates = data
        elif isinstance(data, dict):
            candidates = list(data.get("glossary", {}).keys()) or data.get("terms", [])
        else:
            candidates = []

        filtered = [
            w for w in candidates
            if isinstance(w, str)
            and len(w) >= 2
            and re.fullmatch(r"[\u4e00-\u9fff]+", w)
            and w not in existing_keys
        ]
        filtered.sort(key=lambda s: -len(s))
        return filtered

    except Exception as e:
        logger.error("‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", error=str(e))
        return []


# ---------------------------
# Extract JSON Safely
# ---------------------------
def extract_json_safely(text: str) -> dict:
    stack = []
    start = None
    brackets = {'{': '}', '[': ']'}
    for i, c in enumerate(text):
        if c in '{[':
            if not stack:
                start = i
            stack.append(brackets[c])
        elif c in '}]' and stack:
            if c == stack.pop():
                if not stack and start is not None:
                    try:
                        return json.loads(text[start:i+1])
                    except Exception as e:
                        logger.debug("Failed to parse partial JSON", pos=i, error=str(e))
            else:
                stack.clear()
                start = None
    raise ValueError("No valid JSON found")


# ---------------------------
# Propose translations with cache (‡πÉ‡∏ä‡πâ Gemini)
# ---------------------------
def propose_candidate_translations_json(candidates: List[str]) -> Dict[str,str]:
    results = {}
    unseen = [c for c in candidates if c not in translation_cache]
    for c in candidates:
        if c in translation_cache:
            results[c] = translation_cache[c]

    if not unseen:
        logger.info("‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏°‡∏µ‡πÉ‡∏ô cache ‡πÅ‡∏•‡πâ‡∏ß", count=len(candidates))
        return results

    batch_size = 40

    for i in range(0, len(unseen), batch_size):
        batch = unseen[i:i+batch_size]
        terms_list = "\n".join(batch)
        prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏à‡∏µ‡∏ô‡πÇ‡∏ö‡∏£‡∏≤‡∏ì‡πÅ‡∏ô‡∏ß‡πÑ‡∏ã‡πà‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ "
            "‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ï‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢ "
            "‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ "
            "‡∏´‡πâ‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: "
            "{\"glossary\": {\"Ê±âÂ≠ó\":\"‡πÑ‡∏ó‡∏¢\", ...}}\n\n"
            f"‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå:\n{terms_list}"
        )

        try:
            response_text = translate_with_gemini(
                prompt=prompt,
                model_name=config.model,  # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• default
                max_tokens=1500,
                temperature=0.0
            )

            try:
                data = json.loads(response_text)
            except json.JSONDecodeError:
                data = extract_json_safely(response_text)

            if isinstance(data, dict) and "glossary" in data:
                new_translations = {k.strip(): v.strip() for k,v in data["glossary"].items()}
                results.update(new_translations)
                translation_cache.update(new_translations)
            else:
                logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö glossary ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", raw=response_text[:200])

        except Exception as e:
            logger.error("‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", batch=i//batch_size, error=str(e))

    logger.info("‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•", new=len(results), cached=len(candidates)-len(unseen))
    return results


# ---------------------------
# Smart Chunking (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡πâ‡∏≥)
# ---------------------------
def smart_chunk_with_overlap(text: str, target_tokens: int = None) -> List[str]:
    """
    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunk ‡πÇ‡∏î‡∏¢:
    - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (overlap = 0)
    - ‡πÉ‡∏ä‡πâ‡∏à‡∏∏‡∏î‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏à‡∏µ‡∏ô: „ÄÇÔºÅÔºüÔºõ;
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏Å‡∏•‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
    """
    if target_tokens is None:
        target_tokens = config.chunk_token_target

    if HAVE_TIKTOKEN:
        try:
            enc = tiktoken.encoding_for_model("gpt-4")
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")

        # ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        sentences = re.split(r'(?<=[„ÄÇÔºÅÔºüÔºõ;])', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return [text[:2000]]

        chunks = []
        current_chunk = ""

        for sent in sentences:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö token ‡∏ñ‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            temp_chunk = current_chunk + sent
            token_count = len(enc.encode(temp_chunk))

            if token_count > target_tokens and current_chunk:
                # ‡∏õ‡∏¥‡∏î chunk ‡∏ô‡∏µ‡πâ (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö!)
                chunks.append(current_chunk.strip())
                current_chunk = sent  # ‡πÄ‡∏£‡∏¥‡πà‡∏° chunk ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡∏µ‡πâ
            else:
                current_chunk += sent

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # Safety net: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ chunk ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí ‡πÅ‡∏ö‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        final_chunks = []
        for chunk in chunks:
            if count_tokens(chunk) > target_tokens + 500:
                lines = chunk.split('\n')
                sub_chunk = ""
                for line in lines:
                    if count_tokens(sub_chunk + line) < target_tokens + 800:
                        sub_chunk += line + "\n"
                    else:
                        if sub_chunk:
                            final_chunks.append(sub_chunk.strip())
                        sub_chunk = line + "\n"
                if sub_chunk:
                    final_chunks.append(sub_chunk.strip())
            else:
                final_chunks.append(chunk)

        return final_chunks if final_chunks else [text[:2000]]

    # Fallback
    return [text[i:i+1500] for i in range(0, len(text), 1500)]


# ---------------------------
# Count tokens
# ---------------------------
def count_tokens(text: str) -> int:
    if HAVE_TIKTOKEN:
        try:
            enc = tiktoken.encoding_for_model("gpt-4")
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    return max(1, len(text)//3)


# ---------------------------
# Translate chunk using Gemini (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà)
# ---------------------------
def translate_chunk_using_glossary(chunk_text: str, full_glossary: Dict[str, str], automaton=None) -> Tuple[str, str]:
    if AHO_AVAILABLE and automaton:
        matched_terms = find_all_matched_terms(chunk_text, automaton)
    else:
        matched_terms = {k for k in full_glossary if k in chunk_text}

    relevant_glossary = {k: full_glossary[k] for k in matched_terms}
    glossary_lines = "\n".join(f"{k} | {v}" for k, v in relevant_glossary.items()) if relevant_glossary else "(‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞)"

    prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ‡∏à‡∏µ‡∏ô‡πÇ‡∏ö‡∏£‡∏≤‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ "
        "‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ "
        "‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡∏à‡∏µ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏• ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° "
        "‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏ï‡∏≤‡∏° glossary ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î\n\n"
        f"[Glossary]\n{glossary_lines}\n\n"
        f"[‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö]\n{chunk_text}\n\n"
        "‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô:"
    )

    try:
        translation, used_model = translate_with_model_fallback(
            prompt=prompt,
            preferred_models=config.available_models,
            max_tokens=2000,
            temperature=0.2
        )
        return translation, used_model

    except Exception as e:
        logger.error("‡πÅ‡∏õ‡∏• chunk ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", error=str(e), chunk_preview=chunk_text[:100])
        raise


# ---------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô: ‡∏î‡∏∂‡∏á commit hash ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á glossary
# ---------------------------
def get_current_glossary_commit() -> str:
    try:
        result = subprocess.run(
            ["git", "log", "-1", "--pretty=format:%H", "--", config.glossary_file],
            capture_output=True, text=True
        )
        return result.stdout.strip() if result.returncode == 0 else "unknown"
    except Exception:
        return "unknown"


# ---------------------------
# Save new terms file
# ---------------------------
def save_new_terms_file(chapter_name: str, new_terms: Dict[str,str]):
    fn = os.path.join(config.log_dir, "new_terms", f"new_terms_{chapter_name}_{now_ts()}.txt")
    os.makedirs(os.path.dirname(fn), exist_ok=True)
    with open(fn, "w", encoding="utf-8") as f:
        for k,v in new_terms.items():
            f.write(f"{k} | {v}\n")
    logger.info("Saved new terms", file=fn, count=len(new_terms))


# ---------------------------
# Main function
# ---------------------------
def process_chapter_file(chapter_path: str):
    chapter_name = os.path.splitext(os.path.basename(chapter_path))[0]
    chapter_num = int(re.search(r"(\d+)", chapter_name).group(1)) if re.search(r"(\d+)", chapter_name) else None

    logger.info("Processing chapter", chapter=chapter_name, chapter_num=chapter_num)

    dst_src = os.path.join(config.src_dir, os.path.basename(chapter_path))
    if not os.path.exists(dst_src):
        logger.error("Source file not found", path=dst_src)
        raise FileNotFoundError(f"Not found: {dst_src}")

    with open(dst_src, "r", encoding="utf-8") as f:
        src_text = f.read().strip()

    t0 = time.time()
    glossary = load_glossary()
    automaton = build_automaton_from_dict(glossary) if AHO_AVAILABLE else None

    if automaton:
        matched_keys = find_all_matched_terms(src_text, automaton)
        matches = {k: glossary[k] for k in matched_keys}
    else:
        matches = {k: v for k, v in glossary.items() if k in src_text}

    logger.info("Found existing terms", count=len(matches))

    candidates = extract_candidates(src_text, glossary)
    logger.info("Extracted candidates", count=len(candidates))

    new_translations = propose_candidate_translations_json(candidates)
    logger.info("New translations", count=len(new_translations))

    per_chapter_glossary = {**matches, **new_translations}

    if new_translations:
        save_new_terms_file(chapter_name, new_translations)
        save_glossary(new_translations, chapter_num=chapter_num)

    chunks = smart_chunk_with_overlap(src_text)
    logger.info("Chunking complete", chunks=len(chunks))

    translations = []
    used_models = []
    try:
        from tqdm import tqdm
        chunks_iter = tqdm(chunks, desc=f"‡πÅ‡∏õ‡∏• {chapter_name}", unit="chunk")
    except ImportError:
        chunks_iter = chunks

    for i, ch in enumerate(chunks_iter):
        logger.debug("‡πÅ‡∏õ‡∏• chunk", index=i+1, tokens=count_tokens(ch))
        tr, model_used = translate_chunk_using_glossary(ch, full_glossary=per_chapter_glossary, automaton=automaton)
        translations.append(tr)
        used_models.append(model_used)

    def smart_join_chunks(chunks: List[str]) -> str:
        """‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° chunk ‡πÉ‡∏´‡πâ‡∏•‡∏∑‡πà‡∏ô ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà"""
        if not chunks:
            return ""
    
        result = chunks[0]
        for chunk in chunks[1:]:
            # ‡∏ñ‡πâ‡∏≤ chunk ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‚Üí ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
            if result and result[-1] in "„ÄÇÔºÅÔºüÔºõ;‚Ä¶":
                result += "\n" + chunk
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‚Üí ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ
                result += " " + chunk
        return result

    full_translation = smart_join_chunks(translations)

    out_path = os.path.join(config.out_dir, f"{chapter_name}_translated.txt")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(full_translation)

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata
    meta_dir = os.path.join(config.out_dir, ".meta")
    os.makedirs(meta_dir, exist_ok=True)
    meta_path = os.path.join(meta_dir, f"{chapter_name}.json")

    glossary_commit = get_current_glossary_commit()

    log_record = {
        "timestamp": now_ts(),
        "chapter": chapter_name,
        "source": dst_src,
        "output": out_path,
        "glossary_matches": len(matches),
        "candidates_extracted": len(candidates),
        "new_translations_added": len(new_translations),
        "chunks": len(chunks),
        "duration_sec": round(time.time() - t0, 2),
        "last_glossary_commit": glossary_commit,
        "model_used": used_models[0] if len(set(used_models)) == 1 else used_models,  # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå
    }

    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(log_record, f, ensure_ascii=False, indent=2)

    logger.info("‡πÅ‡∏õ‡∏•‡∏ö‡∏ó‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", chapter=chapter_name, duration=log_record["duration_sec"], output=out_path)
    return out_path


# ---------------------------
# üîπ CLI with argparse (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô batch)
# ---------------------------
def get_files_from_range(start: int, end: int, prefix: str = "sa", src_dir: str = "chapters_src") -> list:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ö‡∏ó"""
    files = []
    for i in range(start, end + 1):
        filename = f"{prefix}_{i:04d}.txt"
        filepath = os.path.join(src_dir, filename)
        if os.path.exists(filepath):
            files.append(filepath)
        else:
            logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå", path=filepath)
    return files

def is_already_translated(chapter_path: str, out_dir: str) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"""
    chapter_name = os.path.splitext(os.path.basename(chapter_path))[0]
    output_file = os.path.join(out_dir, f"{chapter_name}_translated.txt")
    return os.path.exists(output_file)

def main():
    parser = argparse.ArgumentParser(
        description="‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏à‡∏µ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö batch mode",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ‡πÅ‡∏õ‡∏•‡∏ö‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
  python inovel-trans.py chapters_src/sa_0001.txt

  # ‡πÅ‡∏õ‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏ó
  python inovel-trans.py --range 1 10
  python inovel-trans.py --range 1 50 --prefix novel

  # ‡πÅ‡∏õ‡∏•‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß
  python inovel-trans.py --range 1 100 --skip-existing

  # ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á
  python inovel-trans.py --files chapters_src/sa_0001.txt chapters_src/sa_0005.txt
        """
    )
    # ‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏¢‡∏±‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
    parser.add_argument("chapter", nargs="?", help="‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏ä‡πà‡∏ô chapters_src/sa_0001.txt)")
    
    # ‡πÇ‡∏´‡∏°‡∏î batch
    parser.add_argument("--files", nargs="+", help="‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á")
    parser.add_argument("--range", nargs=2, type=int, metavar=("START", "END"), help="‡∏ä‡πà‡∏ß‡∏á‡∏ö‡∏ó ‡πÄ‡∏ä‡πà‡∏ô --range 1 100")
    parser.add_argument("--prefix", default="sa", help="prefix ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (default: sa)")
    
    # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    parser.add_argument("--skip-existing", action="store_true", help="‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß")
    parser.add_argument("--quiet", action="store_true", help="‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô error)")
    
    # config override
    parser.add_argument("--model", help="‡πÇ‡∏°‡πÄ‡∏î‡∏• Gemini (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ comma)", default=None)
    parser.add_argument("--src-dir", help="‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á", default=None)
    parser.add_argument("--out-dir", help="‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á", default=None)
    parser.add_argument("--log-dir", help="‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ logs", default=None)

    args = parser.parse_args()

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï config
    if args.model:
        config.preferred_models = [m.strip() for m in args.model.split(",")]
        config.available_models = get_available_models(config.preferred_models)
        config.model = config.available_models[0]

    if args.src_dir: config.src_dir = args.src_dir
    if args.out_dir: config.out_dir = args.out_dir
    if args.log_dir: config.log_dir = args.log_dir

    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô cache
    init_cache_db()
    load_cache()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå
    if args.range:
        files = get_files_from_range(args.range[0], args.range[1], args.prefix, config.src_dir)
    elif args.files:
        files = []
        for f in args.files:
            if os.path.exists(f):
                files.append(f)
            else:
                logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå", path=f)
    elif args.chapter:
        if os.path.exists(args.chapter):
            files = [args.chapter]
        else:
            logger.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå", path=args.chapter)
            sys.exit(1)
    else:
        parser.print_help()
        sys.exit(1)

    if not files:
        logger.error("‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•")
        sys.exit(1)

    # ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ --skip-existing)
    if args.skip_existing:
        original_count = len(files)
        files = [f for f in files if not is_already_translated(f, config.out_dir)]
        skipped = original_count - len(files)
        if skipped > 0 and not args.quiet:
            logger.info("‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß", count=skipped)

    if not files:
        logger.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏ó‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•")
        sys.exit(0)

    # ‡πÄ‡∏£‡∏¥‡πà‡∏° batch
    total = len(files)
    success_count = 0
    error_count = 0
    error_files = []
    start_time = time.time()

    # --- ‡πÄ‡∏û‡∏¥‡πà‡∏° tqdm progress bar ---
    try:
        from tqdm import tqdm
        chapter_iter = tqdm(files, desc="‡πÅ‡∏õ‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏ó", unit="‡∏ö‡∏ó", disable=args.quiet)
    except ImportError:
        chapter_iter = files
        if not args.quiet:
            logger.info("‡πÑ‡∏°‡πà‡∏û‡∏ö tqdm ‚Äî ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤")

    for chapter_path in chapter_iter:
        chapter_name = os.path.splitext(os.path.basename(chapter_path))[0]
        if not args.quiet and 'tqdm' not in sys.modules:
            logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•: {chapter_name}")

        try:
            process_chapter_file(chapter_path)
            success_count += 1
        except Exception as e:
            error_count += 1
            error_files.append(chapter_path)
            logger.error("‡πÅ‡∏õ‡∏•‡∏ö‡∏ó‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", chapter=chapter_name, error=str(e))

        # Rate limiting: ‡∏´‡∏ô‡πà‡∏ß‡∏á 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ/‡∏ö‡∏ó (‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏ó‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
        if chapter_path != files[-1]:  # ‡πÑ‡∏°‡πà‡∏´‡∏ô‡πà‡∏ß‡∏á‡∏´‡∏•‡∏±‡∏á‡∏ö‡∏ó‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            time.sleep(1.0)

    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    elapsed = time.time() - start_time
    logger.info("="*50)
    logger.info("‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏• batch")
    logger.info("‡πÅ‡∏õ‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", count=success_count)
    logger.info("‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", count=error_count)
    if error_files:
        failed_names = [os.path.splitext(os.path.basename(f))[0] for f in error_files]
        logger.info("‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß", chapters=failed_names)
    logger.info("‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤", seconds=round(elapsed, 1))
    logger.info("="*50)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cache ‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
    save_cache()

    if error_count > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()